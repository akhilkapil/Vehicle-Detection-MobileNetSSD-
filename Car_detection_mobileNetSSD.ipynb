{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\akhil\\\\Desktop\\\\mobilenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNN I am using for this assignment is a caffee version of MobileNet-SSD model, which uses a hybrid of a framework from Google called MobileNet(transfer learning pretrained model) and another framework called Single Shot Detector Multibox.\n",
    "\n",
    "- __MobileNetSSD_deploy.caffemodel__: This is the model.\n",
    "- __MobileNetSSD_deploy.prototxt__: This is the text file that describes the\n",
    "model's parameters.\n",
    "\n",
    "### Why MobileNet SSD ?\n",
    "\n",
    "- __SSD Object Detection__ extracts feature map using a base deep learning network, which are CNN based classifiers, and applies convolution filters to finally detect objects. \n",
    "- My implementation uses MobileNet as the base network (others might include- VGGNet, ResNet, DenseNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 1: Importing models and prototxt file for labels\n",
    "model = cv2.dnn.readNetFromCaffe('MobileNetSSD_deploy.prototxt','MobileNetSSD_deploy.caffemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MobileNet we need to decide some preprocessing parameters that are specified to this model.\n",
    "- It expects the input image to be 300 pixels high\n",
    "- Also, it expects the pixel valuesin the image to be on a scale from -1.0 to 1.0.\n",
    "- This means that, relative to the usual scale from 0 to 255, it is necessary to subtract 127.5 and then divide by 127.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 2: \n",
    "blob_height = 300\n",
    "color_scale = 1.0/127.5\n",
    "average_color = (127.5, 127.5, 127.5)\n",
    "confidence_threshold = 0.5 #Also need to define a confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 3: The model supports 20 classes of objects, with IDs from 1 to 20 (I am using all the classses as it could be \n",
    "## used for real time too to identify different objects). The labels for these classes can be defined as follows:\n",
    "\n",
    "labels = ['airplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "          'car', 'cat', 'chair', 'cow', 'dining table', 'dog',\n",
    "          'horse', 'motorbike', 'person', 'potted plant', 'sheep',\n",
    "          'sofa', 'train', 'TV or monitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 4: For each frame, we need to calculate the aspect ratio.\n",
    "\n",
    "cap = cv2.VideoCapture('1615363610851.mp4') #Put 0 for real time video processing or You can put some other video to check its\n",
    "                                            #performance\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "success, frame = cap.read()\n",
    "color = (255,170,0)\n",
    "while success:\n",
    "    h, w = frame.shape[:2]\n",
    "    aspect_ratio = w/h\n",
    "\n",
    "    # Detect objects in the frame.\n",
    "\n",
    "    blob_width = int(blob_height * aspect_ratio)\n",
    "    blob_size = (blob_width, blob_height)\n",
    "    \n",
    "    #STEP 4: I will be using cv2.dnn.blobFromImage function, with several of its optional arguments, to perform the necessary \n",
    "    #preprocessing, including resizing the frame and converting its pixel data into a scale from -1.0 to 1.0:\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor=color_scale, size=blob_size, mean=average_color)\n",
    "    \n",
    "    # feed the resulting blob to the DNN and get the model's output:\n",
    "    model.setInput(blob)\n",
    "    results = model.forward()  # results are an array, in a format that is specific to the model we are using\n",
    "    \n",
    "    # STEP 5:for object detection DNN trained with the SSD framework â€“ the results include a subarray of detected objects, \n",
    "    #each with its own confidence score, rectangle coordinates, and class ID. The following code shows\n",
    "    #how to access these, as well as how to use an ID to look up a label in the list I defined earlier:\n",
    "    \n",
    "    count = [] #making a empty list to get the count of vehicle \n",
    "    \n",
    "    # Iterate over the detected objects.\n",
    "    for object in results[0, 0]:\n",
    "        confidence = object[2]\n",
    "        if confidence > confidence_threshold:\n",
    "            # Get the object's coordinates.\n",
    "            \n",
    "            x0, y0, x1, y1 = (object[3:7] * [w, h, w, h]).astype(int)\n",
    "            if (x0 <= 1364) & (y0 >=189): #This expression is to satisfy the detetion area condition, which is a line in our case\n",
    "                                          #(vehicle crosses this coordinate our model will detect the object.) \n",
    "                                          #These x0 and y0 coordinates are nothing but to represent the line.\n",
    "                                          #They can be changed according to videos.\n",
    "\n",
    "            # Get the classification result.\n",
    "                id = int(object[1])\n",
    "                label = labels[id - 1]\n",
    "                \n",
    "                #STEP 6:  As we iterate over the detected objects, we draw the detection rectangles, along\n",
    "                # with the classification labels and confidence scores:\n",
    "                \n",
    "                # Draw a blue rectangle around the object.\n",
    "                cv2.rectangle(frame, (x0, y0), (x1, y1),\n",
    "                              (255, 0, 0), 2)\n",
    "                \n",
    "    \n",
    "                # Draw the classification result and confidence.\n",
    "                text = '%s (%.1f%%)' % (label, confidence * 100.0)\n",
    "                cv2.putText(frame, text, (x0, y0 - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                count.append(object)\n",
    "\n",
    "    #The last thing to do with the frame is to show it:\n",
    "    cv2.line(frame, (1,266), (1364,177), color, 2)\n",
    "    cv2.putText(frame, \"vehicles detected: \" + str(len(count)), (889, 19), font, 0.6, (0, 180, 80), 2)\n",
    "    cv2.imshow('Objects', frame)\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:  # Escape\n",
    "        break\n",
    "\n",
    "    success, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
